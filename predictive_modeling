{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NYPD Civilian Complaints\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Shweta Kumar A15409222\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary of Findings\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For my model, I have decided to predict the duration in days a case will take from when its received to when its closed as a regression model with an added column I shall create called \"duration\" that contains the days from when the complaint was received to when it was closed. I will use R^2 scores to evaluate my models and as the metric to improve my model based on."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Baseline Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For my Baseline Model, I will use the bare minimum and train my model with only the features in the table that do not have large unique values that could make the model messy. \n",
    "\n",
    "At the time of prediction, I will not have information on the complainant and officer after the case is closed so I only choose features known when the complaint is filed. \n",
    "\n",
    "For the model, there are 4 categorical variables and 3 numerical variables. There are no ordinal features I used for this baseline model. I implemented a Linear Regression model with a PCA in the categorical features pipeline to eliminate unecessary elements. \n",
    "\n",
    "* R squared = 0.14884089482053664\n",
    "\n",
    "This is not a very high accuracy and I will definitely need to improve upon it in my Final Model. However, it was not negative which means atleast it follows the trend of the data. If it had been negative, it would indicate my model fit worse than a horizontal line. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Final Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For thie final model, I decided to transform the \"outcome_description\" column using a FunctionTransformer that binned each outcome by severity. Any outcome resulting in arrest is labeled as Arrest, and ending in summons is labeled as Summons, and all others are Other. Then, I used OHE to create multiple more features to enhance my model. Aside from my engineered feature, the comoplainant's gender and age were features I decided to add on as well as my EDA from my previous project indicated trends with the complainant's demographic and how long it took to process the complaint. \n",
    "\n",
    "I experimented with DecisionTreeRegressor, KNeighborsRegressor, and RandomForestRegressor and found that RandomForestRegressor worked best with my model looking at my R^2 values. I then used GridSearch to figure which parramters would be best for my model and tuned my paramters. However, my default parameters actually ended up working better than those I got from the Search so I kept my default params. \n",
    "\n",
    "* R squared = 0.81928371847372\n",
    "\n",
    "My R squared value indicates significant improvement in accuracy from my Baseline model so I was pleased. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fairness Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I decided to use the subset of Male vs Female cops the complainant filed against to assess the fairness of my model. I set the significance level to 0.01 and performed a permuation test using r squared as my parity measure as I had done previously. I got a value above my significance level so I fail to reject my null that the model is fair towards cops of both genders. I used R^2 as my parity measure because that is the best measure of accuracy given my model is not a binary classification. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Import all necessary packages\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import os\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import datetime\n",
    "%matplotlib inline\n",
    "%config InlineBackend.figure_format = 'retina'  # Higher resolution figures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.preprocessing import FunctionTransformer\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.tree import DecisionTreeClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#load the dataset\n",
    "allegations_fp = os.path.join('data', 'allegations.csv')\n",
    "allegations = pd.read_csv(allegations_fp)\n",
    "#Will not be using these columns for any sort oof analysis so I want to just drop them for now\n",
    "allegations= allegations.drop([\"unique_mos_id\",\"first_name\",\"last_name\",\"shield_no\"], axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Creating the \"duration\" column \n",
    "allegations = allegations.rename(columns={\"month_received\": \"month\", \"year_received\": \"year\"})\n",
    "allegations['date_received'] = pd.to_datetime(allegations[['year', 'month']].assign(day=1))\n",
    "allegations = allegations.drop([\"month\",\"year\"], axis = 1)\n",
    "allegations = allegations.rename(columns={\"month_closed\": \"month\", \"year_closed\": \"year\"})\n",
    "allegations['date_closed'] = pd.to_datetime(allegations[['year', 'month']].assign(day=1))\n",
    "allegations = allegations.drop([\"month\",\"year\"], axis = 1) #drop month, year but have new datetime columns\n",
    "\n",
    "\n",
    "allegations[\"duration\"] = allegations[\"date_closed\"] - allegations[\"date_received\"] #calculate total days duration\n",
    "allegations[\"duration\"] = allegations[\"duration\"] / np.timedelta64(1,'D')\n",
    "allegations[\"duration\"] = allegations[\"duration\"].apply(lambda x: int(x))\n",
    "\n",
    "allegations[\"allegation\"] = allegations[\"allegation\"].str.lower() #standardize allegations column"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# BASELINE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.14792343530663699"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X = allegations.drop(\"duration\", axis =1) #drop the predictor\n",
    "y = allegations['duration']\n",
    "types = X.dtypes\n",
    "catcols = [\"rank_abbrev_incident\",\"rank_incident\",\"mos_ethnicity\",\"mos_gender\"] #bare minimum categorical variables\n",
    "numcols = [\"complainant_age_incident\",\"precinct\",\"mos_age_incident\"]\n",
    "cats = Pipeline([\n",
    "    ('imp', SimpleImputer(strategy='constant', fill_value='NULL')),\n",
    "    ('ohe', OneHotEncoder(handle_unknown='ignore', sparse=False)),\n",
    "    ('pca', PCA(svd_solver='full', n_components=0.99))]) #elimnate unnecessary elements\n",
    "    \n",
    "\n",
    "ct = ColumnTransformer([\n",
    "    ('catcols', cats, catcols),\n",
    "    ('numcols', SimpleImputer(strategy='constant', fill_value=0), numcols) #impute null values in numerical columns\n",
    "])\n",
    "\n",
    "pl = Pipeline([('feats', ct), ('reg', LinearRegression())])\n",
    "X_tr, X_ts, y_tr, y_ts = train_test_split(X, y, test_size=0.25)\n",
    "pl.fit(X_tr, y_tr)\n",
    "pl.score(X_ts, y_ts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# FINAL MODEL"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using FunctionTransformer to bin outcomes by severity and OHE the results for my new feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.81928371847372\n"
     ]
    }
   ],
   "source": [
    "def outcome_bin(X):\n",
    "    def outcome_helper (column):\n",
    "        if type(column) == float: \n",
    "            return \"Other\"\n",
    "        if \"Arrest\" in column: \n",
    "            return \"Arrest\"\n",
    "        if \"Summons - \" in column: \n",
    "            return \"Summons\"\n",
    "        if \"Moving violation summons issued\" in column: \n",
    "            return \"Summons\"\n",
    "        if \"Parking summons issued\" in column: \n",
    "            return \"Summons\"\n",
    "        return \"Other\"\n",
    "    return pd.DataFrame(X[\"outcome_description\"].apply(outcome_helper))\n",
    "\n",
    "num_feat = ['outcome_description']\n",
    "num_transformer = Pipeline([('func-trans', FunctionTransformer(func = outcome_bin)),\n",
    "                            ('ohe', OneHotEncoder(handle_unknown='ignore',sparse=False))])\n",
    "\n",
    "catcols = [\"rank_abbrev_incident\",\"rank_incident\",\"mos_ethnicity\"\n",
    "            ,\"complainant_ethnicity\",\"mos_gender\",\"complainant_gender\"]\n",
    "\n",
    "numcols = [\"mos_age_incident\", \"complainant_age_incident\",\"precinct\"]\n",
    "\n",
    "cats = Pipeline([\n",
    "    ('imp', SimpleImputer(strategy='constant', fill_value='NULL')),\n",
    "    ('ohe', OneHotEncoder(handle_unknown='ignore', sparse=False)),\n",
    "    ('pca', PCA(svd_solver='full', n_components=0.99))]) #elimnate unnecessary elements\n",
    "\n",
    "\n",
    "preproc = ColumnTransformer(transformers=[('numcols', SimpleImputer(strategy='constant', fill_value=0),numcols),\n",
    "                                          ('num', num_transformer, num_feat),\n",
    "                                          ('cat', cats, catcols)], remainder=\"drop\")\n",
    "\n",
    "pl = Pipeline(steps=[('preprocessor', preproc), ('clf', RandomForestRegressor())])\n",
    "\n",
    "X = allegations.drop(['fado_type','allegation','contact_reason','date_closed','date_received','duration'], axis=1)\n",
    "y = allegations['duration']\n",
    "\n",
    "X_tr, X_ts, y_tr, y_ts = train_test_split(X, y, test_size=0.25)\n",
    "pl.fit(X_tr, y_tr)\n",
    "preds = pl.predict(X_ts)\n",
    "print(pl.score(X_ts, y_ts))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#dictionary of parameters to possible tune\n",
    "parameters = {\n",
    "    'clf__max_depth': [2,3,4,5,7,10,30,75,100,None], 'clf__max_features':[\"sqrt\",\"auto\",\"log2\"],\n",
    "    'clf__min_samples_split':[2,3,5,7,10,15,20],\n",
    "    'clf__min_samples_leaf':[2,3,5,7,10,15,20]\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Use a GridSearch to determine best possible params for the RandomForestRegressor\n",
    "X = allegations.drop(\"duration\", axis = 1)\n",
    "y = allegations['duration']\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3)\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "clf = GridSearchCV(pl, parameters, cv = 5) #train with pipeline\n",
    "#clf.fit(X_train, y_train)\n",
    "#clf.best_params_\n",
    "\n",
    "#GridSearch params produced lower accuracy than default params so I keep the default"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# FAIRNESS EVALUATION"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Subset = Males vs Female Cops"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-50-9adbfa6e1fd1>:5: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  results[\"prediction\"] = preds\n",
      "<ipython-input-50-9adbfa6e1fd1>:6: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  results[\"duration\"] = y_ts\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.02\n"
     ]
    }
   ],
   "source": [
    "#Significance = 0.01\n",
    "from sklearn import metrics\n",
    "from sklearn.metrics import r2_score\n",
    "\n",
    "results = X_ts\n",
    "results[\"prediction\"] = preds\n",
    "results[\"duration\"] = y_ts\n",
    "\n",
    "obs = r2_score(y_ts, preds)\n",
    "\n",
    "metrs = []\n",
    "for _ in range(100):\n",
    "    s = (\n",
    "        results[['mos_gender', 'prediction', 'duration']]\n",
    "        .assign(mos_gender=results.mos_gender.sample(frac=1.0, replace=False).reset_index(drop=True))\n",
    "        .groupby('mos_gender')\n",
    "        .apply(lambda x: r2_score(x.duration, x.prediction))\n",
    "        .iloc[-1]\n",
    "    )\n",
    "    \n",
    "    metrs.append(s)\n",
    "print(pd.Series(metrs <= obs).mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
